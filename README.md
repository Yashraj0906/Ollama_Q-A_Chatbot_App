ğŸ§  Enhanced Q&A Chatbot with Ollama

This is a Streamlit web app powered by LangChain and Ollama that allows users to interact with locally hosted open-source LLMs like Gemma, Phi-3, and Gemma-3:1B.
It provides a simple question-and-answer interface with adjustable parameters such as temperature and token limit.

ğŸš€ Features

ğŸ’¬ Interactive chatbot using LangChainâ€™s prompt pipeline

âš™ï¸ Choose Ollama models dynamically (e.g., gemma2, phi3, gemma3:1b)

ğŸ”¥ Adjustable temperature and max token sliders

ğŸ§© Uses LangChain for structured prompts and output parsing

âš¡ Runs locally using Ollama â€” no external API key required

ğŸ§  Optional integration with LangSmith for model tracing and debugging

ğŸ—‚ï¸ Project Structure
ğŸ“ GenAI Projects/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ .env                  # Environment variables (optional)
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md              # Project documentation

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the repository
git clone (https://github.com/Yashraj0906/Ollama_Q-A_Chatbot_App.git)
cd Ollama_Chatbot_App

2ï¸âƒ£ Create a virtual environment
python -m venv .venv


Activate it:

Windows: .\.venv\Scripts\activate

macOS/Linux: source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Install and run Ollama

Download Ollama: https://ollama.com/download

Then pull your desired model:

ollama pull gemma2
# or
ollama pull phi3
# or
ollama pull gemma3:1b


Verify installation:

ollama list

5ï¸âƒ£ Run the app
streamlit run app.py
